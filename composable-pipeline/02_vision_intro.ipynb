{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Image Processing\n",
    "----\n",
    "\n",
    "This notebook introduces some of the image processing algorithms/functions that are used in the composable pipeline. \n",
    "\n",
    "## Aims\n",
    "* Introduce image algorithms\n",
    "\n",
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [OpenCV](#opencv)\n",
    "* [Color Conversion](#colorconv)\n",
    "* [2D Convolution](#2dconv)\n",
    "* [Morphological Transformations](#morphological)\n",
    "* [Corner Detector](#corner)\n",
    "* [Fork Operation](#fork)\n",
    "* [Join Operations](#join)\n",
    "* [Color Thresholding](#thresh)\n",
    "* [Look Up Table](#lut)\n",
    "* [Takeaways](#takeaways)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    "----\n",
    "\n",
    "## Revision History\n",
    "\n",
    "* v1.0 | 15 April 2021 | First notebook revision.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "### What is a pixel?\n",
    "\n",
    "[Merriam Webster](https://www.merriam-webster.com/dictionary/pixel) defines pixel as:\n",
    "> any of the small discrete elements that together constitute an image\n",
    "\n",
    "Usually pixels are built of channels, in the case of color images there are three channels Red, Blue and Green (RGB). Each pixel is a discrete representation of the light intensity for a particular channel. The light intensity is encoded in 8-bit, hence only 256 discrete values are possible per channel. \n",
    "\n",
    "Run the next cell and then click on the blue box. Interact with the color picker and observe the light intensity for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "widgets.ColorPicker(\n",
    "    concise=False,\n",
    "    description='Pick a color',\n",
    "    value='blue',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many colors can a pixel represent?\n",
    "\n",
    "A color pixel is built of three channels, each pixel channel can represent 256 values. So, a color pixel can represent $256 \\times 256 \\times 256 = 16,777,256$ different colors.\n",
    "\n",
    "### What is an image?\n",
    "\n",
    "An image is a collection of pixel, where each pixel store a value proportional to the light intensity at that particular location. The size of an image is its dimension (or resolution) which is specified in width and hight. For instance, $1920x1080$\n",
    "\n",
    "### What does the term frames per second refer to?\n",
    "\n",
    "In a video the term frames per second (FPS), or images per second, represents the number of images that are played every second. The higher the image resolution and/or FPS the better the hardware required to record, process and reproduce video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV <a class=\"anchor\" id=\"opencv\"></a>\n",
    "\n",
    "[OpenCV](https://opencv.org/) is an open source cross platform computer vision library. You will be using OpenCV to get familiar with some of the computer vision functions.\n",
    "\n",
    "Import numpy, OpenCV and matplotlib to visualize images. We will use a mosaic image to demonstrate the effect of some vision operation to such image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('img/mosaic.jpg')\n",
    "plt.figure(figsize=(10, 10)), plt.axis(\"off\"), plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Conversion <a class=\"anchor\" id=\"colorconv\"></a>\n",
    "\n",
    "A color image is represented on the [RGB color space](https://en.wikipedia.org/wiki/RGB_color_space), however there are many different color spaces. Each of them have a particular purpose. We will explore some of them\n",
    "\n",
    "### Grayscale\n",
    "\n",
    "A [grayscale image](https://en.wikipedia.org/wiki/Grayscale) has only one channel which represents the amount of light that each pixel contains. One of the main purposes of grayscale images on vision applications is to detect edges on images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "plt.figure(figsize=(10, 10)), plt.axis(\"off\"), plt.imshow(gray, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSV Color space\n",
    "\n",
    "[HSV color space](https://en.wikipedia.org/wiki/HSL_and_HSV) represents an image using the channels hue, saturation and value. This color space aligns a bit better to the way human perceives color-making attributes, and in vision application this color space is useful to detect color more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "plt.figure(figsize=(10, 10)), plt.axis(\"off\"), plt.imshow(hsv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XYZ Color space\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/CIE_1931_color_space):\n",
    ">  The CIE 1931 XYZ color space defines quantitative links between distributions of wavelengths in the electromagnetic visible spectrum, and physiologically perceived colors in human color vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = cv.cvtColor(img, cv.COLOR_BGR2XYZ)\n",
    "plt.figure(figsize=(10, 10)), plt.axis(\"off\"), plt.imshow(xyz);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Convolution <a class=\"anchor\" id=\"2dconv\"></a>\n",
    "\n",
    "It is also know as [filter2D](https://docs.opencv.org/3.4.3/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04) on OpenCV.\n",
    "\n",
    "The 2D convolution is a mathematical operations that uses a kernel (matrix of a dimension $n \\times n$). This kernel slides over the input image and produces an output image.\n",
    "\n",
    "In the next few cells we will consider a few $3 \\times 3$ kernels. You can explore more kernels live [here](https://setosa.io/ev/image-kernels/)\n",
    "\n",
    "### Identity kernel\n",
    "\n",
    "The output of a 2D convolution using an identity kernel is the same image.\n",
    "\n",
    "$\n",
    "Identity = \\begin{bmatrix}\n",
    "0 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = np.array([[0,0,0],[0,1,0],[0,0,0]],np.float32)\n",
    "identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the identity kernel to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = cv.filter2D(img,-1,identity)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Identity')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Are image the same? {}\".format(np.array_equal(img,dst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emboss kernel\n",
    "\n",
    "The output of a 2D convolution using an emboss kernel produces an image that stress the difference of pixels in a given direction given an illusion of depth\n",
    "\n",
    "$\n",
    "Emboss = \\begin{bmatrix}\n",
    "-2 & -1 & 0\\\\\n",
    "-1 & 1 & 1\\\\\n",
    "0 & 1 & 2\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emboss = np.array([[-2,-1,0],[-1,1,1],[0,1,2]],np.float32)\n",
    "emboss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = cv.filter2D(img,-1,emboss)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Emboss')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Transformations <a class=\"anchor\" id=\"morphological\"></a>\n",
    "\n",
    "Morphological transformations are operations based on the image shape. The used kernel decides the nature of operation. \n",
    "\n",
    "### Dilate\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Dilation_(morphology)):\n",
    "> The dilation operation usually uses a structuring element for probing and expanding the shapes contained in the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3,3),np.uint8)\n",
    "dilate = cv.dilate(img,kernel,iterations = 1)\n",
    "plt.figure(figsize=(10, 10)), plt.axis(\"off\"), plt.imshow(dilate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erode\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Erosion_(morphology)):\n",
    "> The erosion operation usually uses a structuring element for probing and reducing the shapes contained in the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3,3),np.uint8)\n",
    "erode = cv.erode(img,kernel,iterations = 1)\n",
    "plt.figure(figsize=(10, 10)), plt.axis(\"off\"), plt.imshow(erode);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner Detector  <a class=\"anchor\" id=\"corner\"></a>\n",
    "\n",
    "### Harris Corner Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = np.float32(cv.cvtColor(img,cv.COLOR_BGR2GRAY))\n",
    "harris = cv.cornerHarris(gray,2,3,0.1)\n",
    "plt.figure(figsize=(10, 10)), plt.axis(\"off\"), plt.imshow(harris, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can overlap the result on top of the original image, red dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = cv.dilate(harris, None)\n",
    "corners = img.copy()\n",
    "corners[dst>0.01*dst.max()]=[255,0,0]\n",
    "plt.figure(figsize=(10, 10)), plt.axis(\"off\"), plt.imshow(corners);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = cv.FastFeatureDetector_create()\n",
    "kp = fast.detect(img, None)\n",
    "img2 = cv.drawKeypoints(img, kp, None, color=(255,0,0))\n",
    "plt.figure(figsize=(10, 10)), plt.axis(\"off\"), plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fork Operation <a class=\"anchor\" id=\"fork\"></a>\n",
    "\n",
    "### Duplicate\n",
    "\n",
    "This operation simply produces two copies of the same image, in the FPGA these two images will be processed at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = img.copy()\n",
    "img3 = img.copy()\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "plt.subplot(131),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(132),plt.imshow(img2),plt.title('Copy 1')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(133),plt.imshow(img3),plt.title('Copy 2')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Operations <a class=\"anchor\" id=\"join\"></a>\n",
    "\n",
    "These vision operations take two input images and produce a single one as a result\n",
    "\n",
    "We will generate the input images as the mosaic version after erode and dilate operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3,3),dtype=np.uint8)\n",
    "dilate = cv.dilate(img,kernel,iterations = 1)\n",
    "erode = cv.erode(img,kernel,iterations = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract\n",
    "\n",
    "This function perform a pixel-wise subtraction, therefore the order of the the images matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = erode - dilate\n",
    "de = dilate - erode\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121),plt.imshow(ed),plt.title('erode - dilate')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(de),plt.title('dilate - erode')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = cv.absdiff(erode, dilate)\n",
    "de = cv.absdiff(dilate, erode)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121),plt.imshow(ed),plt.title('erode - dilate')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(de),plt.title('dilate - erode')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Are image the same? {}\".format(np.array_equal(ed,de)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add\n",
    "\n",
    "The addition of two images can potentially lead to overflow (the result is bigger than 255). Depending on the implementation the result can either\n",
    "1. Saturate: you will notice large white areas in the image\n",
    "1. Wrap around: you will notice artifacts in the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addsat = cv.add(erode, dilate)\n",
    "addwrap = erode + dilate\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121),plt.imshow(addsat),plt.title('Add Saturate')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(addwrap),plt.title('Add wrap around')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Thresholding <a class=\"anchor\" id=\"thresh\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "mask = cv.inRange(img, (0, 100, 0), (80,255,80))\n",
    "mask_rgb = cv.cvtColor(mask,cv.COLOR_GRAY2BGR)\n",
    "cothr = img & mask_rgb\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(cothr),plt.title('Color Detect')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look Up Table <a class=\"anchor\" id=\"lut\"></a>\n",
    "\n",
    "This function performs pixel-wise look-up table transformation. This is, output image is a map of the input image using a 256 table. The light intensity of each channel is used as address and the content is the output result.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>INFO:</strong> The compute vision function LUT does not refer to an FPGA LUT. Although, the concept in essence is quite similar.\n",
    "</div>\n",
    "\n",
    "\n",
    "Let see this function in action\n",
    "\n",
    "### Negative\n",
    "\n",
    "Produce the negative of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = np.zeros((256,1),dtype=np.uint8)\n",
    "for i in range(len(lut)):\n",
    "    lut[i] = 255-i\n",
    "\n",
    "dst = cv.LUT(img, lut)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Negative')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding\n",
    "\n",
    "Produce an output image that lights up pixels with light intensity within a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = np.zeros((256,1),dtype=np.uint8)\n",
    "for i in range(len(lut)):\n",
    "    if 188 > i > 127:\n",
    "        lut[i] = 255\n",
    "\n",
    "dst = cv.LUT(img, lut)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Thresholded')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways <a class=\"anchor\" id=\"takeaways\"></a>\n",
    "\n",
    "After having read and used this notebook you should be able to answer the following questions:\n",
    "\n",
    "- What is a pixel?\n",
    "- What is an image?\n",
    "- What is image resolution?\n",
    "- What is frames per second?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a class=\"anchor\" id=\"conclusion\"></a>\n",
    "\n",
    "This notebooks provides a brief introduction to core concepts of computer vision. The OpenCV library is introduced and used to visualize some of the common computer vision functionality.\n",
    "\n",
    "[⬅️ Getting Started with the Composable Pipeline](01_get_started.ipynb) | | [Difference of Gaussians Application ➡️](applications/01_difference_gaussians_app.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2021 Xilinx, Inc\n",
    "\n",
    "SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
